# Классификация токсичных комментариев

Проект направлен на разработку модели, способной автоматически определять токсичность пользовательских комментариев. Такая система может использоваться в службах модерации, сервисах с пользовательским контентом и в автоматических фильтрах качества коммуникации.

---

## Цели исследования
- Проанализировать корпус текстов и выявить особенности токсичных и нетоксичных комментариев.  
- Подготовить данные: очистить, нормализовать и лемматизировать текст.  
- Исследовать лексическое различие между классами.  
- Сформировать признаковое пространство на основе TF-IDF.  
- Обучить и сравнить несколько моделей.  
- Выбрать финальную модель, удовлетворяющую требованию F1 ≥ 0.75.  

---

## Данные
Исходный датасет содержит:
- **159 292** комментария,  
- столбцы: `text` (текст комментария), `toxic` (целевой признак: токсичность),  
- пропуски и дубликаты отсутствуют.

Выборка выраженно несбалансирована: большинство комментариев относятся к классу 0 (нетоксичные).

---

## Предобработка данных
Выполнены следующие этапы:
- очистка текста от лишних символов;  
- приведение к нижнему регистру;  
- удаление стоп-слов;  
- лемматизация с использованием spaCy;  
- фильтрация коротких токенов;  
- формирование нового признака `lemm_text`;  
- векторизация текстов методом TF-IDF (1–2-граммы, до 50 000 признаков).

---

## Исследовательский анализ данных (EDA)
В ходе анализа:
- изучено распределение классов;  
- выявлен значительный дисбаланс в пользу нетоксичных комментариев;  
- проведено сравнение частот слов в токсичных и нетоксичных текстах;  
- построены словари наиболее употребляемых слов для каждого класса.

Основные наблюдения:
- токсичные комментарии содержат ограниченный набор грубых повторяющихся выражений;  
- нетоксичные тексты более разнообразны по лексике и связаны преимущественно с обсуждением содержания статей и процессов редактирования.

---

## Обучение моделей
В рамках исследования протестированы:
- **Logistic Regression** с балансировкой классов;  
- **LinearSVC** с такими же условиями.

Оценивание проводилось с использованием стратифицированной кросс-валидации по F1-метрике.

Результаты:
- Logistic Regression: F1 = **0.7519**,  
- LinearSVC: F1 = **0.7569**.

LinearSVC продемонстрировала лучшую устойчивость и была выбрана финальной моделью.

---

## Финальная модель
Модель **LinearSVC** обучена на полной тренировочной выборке и протестирована на отложенном наборе.

Ключевые результаты:
- **Итоговый F1:** 0.7563, что выше минимального требования задачи;  
- для нетоксичных комментариев показатели очень высокие (precision и recall выше 0.95);  
- для токсичных — показатели ниже, но приемлемые (F1 ≈ 0.756), модель эффективно находит токсичные примеры.

Особенность поведения:
- модель склонна чаще предсказывать токсичность, что увеличивает recall для класса 1 — важное свойство для систем модерации.

---

## Используемый стек
- Python  
- Pandas, NumPy  
- Matplotlib, Seaborn  
- spaCy  
- NLTK  
- WordCloud  
- Scikit-learn (TfidfVectorizer, LogisticRegression, LinearSVC, StratifiedKFold, cross-validation tools)  
- Регулярные выражения (`re`)

---

## Основные выводы
- Данные успешно подготовлены: выполнена очистка, нормализация и лемматизация.  
- Токсичные комментарии действительно имеют ограниченную лексику и содержат ярко выраженные оскорбления.  
- Наиболее информативным способом представления текстов стала TF-IDF матрица с 1–2-граммами.  
- Сравнение моделей показало, что LinearSVC лучше всего справляется с задачей.  
- Итоговый F1 превышает требуемый уровень, что позволяет использовать модель в реальных задачах фильтрации токсичности.

